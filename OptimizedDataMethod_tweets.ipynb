{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "776b7cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tweepy\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2514df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hello, This is Xiaoyu Wei, and I want to share what I was doing with these codes.^^ \n",
    "\n",
    "# In the beginning, we directly chose the dataset on the IEEE official website, which is the daily Twitter dataset since the outbreak of the Covid-19, \n",
    "# equivalent to more than 900 days and more than 1 million tweets per day. This IEEE dataset has only Twitter token number and no Twitter text body information. \n",
    "# We need to use algorithm to turn the Twitter number into text and then perform sentiment analysis.\n",
    "\n",
    "# Patrick was excellent and wrote the code on how to make the Twitter token number into regular text, \n",
    "# But the algorithm to translate one numbers file into Tweets text took 9 hours! (remember, we need to do with 900+ files! 900*9h ... ) \n",
    "# (His work is not trivial, he is smart. 9 hours' algorithm has already been optimized ! )\n",
    "\n",
    "# So, I rewrote the codes and spent 3 days and 3 nights constantly optimizing them (I wrote the codes during the day and tested them at night to see\n",
    "# if they would report errors, I needed to wake up several times in the middle of the night to see if report errors....) \n",
    "# and finally, \n",
    "# this version of the algorithm downside can translate 1 file (with over 1 million tweets)  in less than 1 hour ! Big progress!\n",
    "\n",
    "# until now, there's no people on the internet, GitHub, have achieved a similar job before.\n",
    "# If other research teams in the future need to analyze the IEEE Covid-19 Twitter data, this optimized method code can significantly improve their work efficiency.;)\n",
    "\n",
    "# .....But Since we used other already well-done datasets that Patrick found later, these efforts could have been more decisive in our paper. \n",
    "# But I learned a lot from these optimization attempts!! ;D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71aea3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = \"aRmOhzcdCDiWdVts9Q0K45u5K\"\n",
    "consumer_secret = \"TFsYHY5MBfMfzlXy33vsuttCjIzdKtKQB6Nalq3X2NFrfiw8m3\"\n",
    "access_token = \"1573003732987613187-ODn0j3HDW0BaYNJZll0DEvhODhGAZs\"\n",
    "access_token_secret = \"DFeLQprKhBQWj6VWth7DnMxoBYUwZhYiC4Jd3LBoSpcyw\"\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "keywords = [\"vaccine\", \"vax\", \"vacc\", \"sinopharm\", \"sputnik\", \"pfizer\", \"moderna\", \"astrazeneca\",\n",
    "            \"vaxzevria\", \"spikevax\", \"novavax\", \"nuvaxovid\", \"biontech\", \"comirnaty\",\n",
    "            \"bbil\", \"covaxin\", \"janssen\", \"johnson\", \"covishield\", \"BBIBP-CorV\",\n",
    "            \"sinovac\", \"coronavac\", \"gamaleya\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e724f762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_files(dataframe, batch_size):\n",
    "    last_tweet_num = 0\n",
    "    counter = 0\n",
    "    length = len(dataframe)\n",
    "    batch_number = int(length/batch_size)\n",
    "    num_left_over = length-batch_number*batch_size\n",
    "    for i in range(0, len(dataframe), batch_size):\n",
    "        cur_batch = dataframe[i:i+batch_size]\n",
    "        try:          \n",
    "            statuses = api.lookup_statuses(cur_batch, tweet_mode='extended')\n",
    "        except:\n",
    "            print(\"---Need sometime to make next fetch---\")\n",
    "            time.sleep(600)\n",
    "            print(\"---Restart Fetching---\")\n",
    "            statuses = api.lookup_statuses(cur_batch, tweet_mode='extended')\n",
    "            \n",
    "        for status in statuses:\n",
    "            if(status.full_text is not None):\n",
    "                tweet_text = status.full_text\n",
    "                tweet_time = status.created_at\n",
    "                tweet_coords = status.coordinates\n",
    "                tweet_place = status.place\n",
    "                tweet_id = status.id\n",
    "\n",
    "                contains_keyword = False\n",
    "\n",
    "                for keyword in keywords:\n",
    "                    if(keyword.lower() in tweet_text.lower()):\n",
    "                        contains_keyword = True\n",
    "                \n",
    "                if(contains_keyword):\n",
    "                    outfile = open(\"./new_tweet_\" + str(last_tweet_num) + \".txt\", \"w\")\n",
    "                    outfile.write(str(tweet_text))\n",
    "                    outfile.write(\"\\n\\n\")\n",
    "                    outfile.write(str(tweet_time))\n",
    "                    outfile.write(\"\\n\\n\")\n",
    "                    outfile.write(str(tweet_coords))\n",
    "                    outfile.write(\"\\n\\n\")\n",
    "                    outfile.write(str(tweet_place))\n",
    "                    outfile.write(\"\\n\\n\")\n",
    "                    outfile.write(str(tweet_id))\n",
    "\n",
    "                    outfile.close()\n",
    "                    last_tweet_num += 1\n",
    "\n",
    "        counter = counter+1\n",
    "        \n",
    "    for i in reversed(range(len(dataframe)-num_left_over,len(dataframe))):\n",
    "        cur_batch = dataframe[batch_size*batch_number:len(dataframe)]\n",
    "        try:          \n",
    "            statuses = api.lookup_statuses(cur_batch, tweet_mode='extended')\n",
    "        except:\n",
    "            print(\"---Need sometime to make next fetch---\")\n",
    "            time.sleep(600)\n",
    "            print(\"---Restart Fetching---\")\n",
    "            statuses = api.lookup_statuses(cur_batch, tweet_mode='extended')\n",
    "            \n",
    "        for status in statuses:\n",
    "            if(status.full_text is not None):\n",
    "                tweet_text = status.full_text\n",
    "                tweet_time = status.created_at\n",
    "                tweet_coords = status.coordinates\n",
    "                tweet_place = status.place\n",
    "                tweet_id = status.id\n",
    "\n",
    "                contains_keyword = False\n",
    "\n",
    "                for keyword in keywords:\n",
    "                    if(keyword.lower() in tweet_text.lower()):\n",
    "                        contains_keyword = True\n",
    "\n",
    "                if(contains_keyword):\n",
    "                    name = \"new_tweet_\" + str(last_tweet_num) + \".txt\"\n",
    "                    outfile = open(\"./tweets/new_tweets/new_tweet_\" + str(last_tweet_num) + \".txt\", \"w\")\n",
    "\n",
    "                    outfile.write(str(tweet_text))\n",
    "                    outfile.write(\"\\n\\n\")\n",
    "                    outfile.write(str(tweet_time))\n",
    "                    outfile.write(\"\\n\\n\")\n",
    "                    outfile.write(str(tweet_coords))\n",
    "                    outfile.write(\"\\n\\n\")\n",
    "                    outfile.write(str(tweet_place))\n",
    "                    outfile.write(\"\\n\\n\")\n",
    "                    outfile.write(str(tweet_id))\n",
    "\n",
    "                    outfile.close()\n",
    "                    last_tweet_num += 1\n",
    "        counter = counter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdbe635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_and_tweets(path):\n",
    "    extension = 'csv'\n",
    "    files_names = glob.glob('*.{}'.format(extension))\n",
    "    current_path = os.getcwd()\n",
    "    print(current_path)\n",
    "    for i in range(len(files_names)):\n",
    "\n",
    "        cur_dataframe = pd.read_csv(files_names[i])\n",
    "        print(\"---Creating folder for\",files_names[0][:-4], \"---\")\n",
    "        if not os.path.exists(files_names[0][:-4]):\n",
    "            os.mkdir(files_names[0][:-4])\n",
    "        else:\n",
    "            print(\"---Folder,\", files_names[0][:-4], \"Already Exists!---\")\n",
    "       \n",
    "        print(\"---Changing Directory---\")\n",
    "        os.chdir(files_names[i][:-4])\n",
    "        \n",
    "        \n",
    "        \n",
    "        sample_ids = list(cur_dataframe.iloc[:,0])\n",
    "        print(\"---Fetching Tweets...---\")\n",
    "        generate_text_files(sample_ids, batch_size=100)\n",
    "        print(\"---Compeleting Fetching Tweets for, \", files_names[i], \"---\")\n",
    "        print(\"---Returning to parent directory---\")\n",
    "        \n",
    "        os.chdir('C:\\\\Users\\\\xywei\\\\Desktop\\\\Xiaoyu DataScience\\\\tweets\\\\')\n",
    "    \n",
    "    print(\"---Done Fetching---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94294c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\xywei\\\\Desktop\\\\Xiaoyu DataScience\\\\tweets\\\\')\n",
    "get_id_and_tweets('./tweets/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
